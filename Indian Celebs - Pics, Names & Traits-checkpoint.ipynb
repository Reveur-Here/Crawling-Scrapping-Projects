{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from csv import writer\n",
    "from csv import reader\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information on a particular Celebrity of our choice from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page=int(input(\"Enter How many pages you want to search: \"))\n",
    "def scrape(max_pages):\n",
    "    j=page\n",
    "    while j <= max_pages:\n",
    "        my_url = 'https://www.imdb.com/list/ls068010962/?sort=list_order,asc&mode=detail&page=' + str(j)\n",
    "        uClient = uReq(my_url)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "        img=[]\n",
    "        nm=[]\n",
    "        des=[]\n",
    "        wiki=[]\n",
    "        \n",
    "        page_soup = soup(page_html, \"html.parser\")\n",
    "        images = page_soup.findAll(\"div\", {\"class\":\"lister-item-image\"})\n",
    "        titles = page_soup.findAll(\"div\", {\"class\":\"lister-item-content\"})\n",
    "        names = page_soup.findAll(\"h3\", {\"class\":\"lister-item-header\"})\n",
    "        descriptions= page_soup.findAll(\"div\",{\"class\":\"lister-item-content\"})\n",
    "        \n",
    "        for i in range(len(titles)):\n",
    "            description= descriptions[i]\n",
    "            x=description.findAll(\"p\")\n",
    "            y=x[1].get_text()[5:-1]\n",
    "            z=y.replace(\"\\n\",\"\")\n",
    "            des.append(z.replace(\",\",\"\"))\n",
    "            image=images[i]\n",
    "            image_source=image.a.img[\"src\"]\n",
    "            img.append(image_source.replace(\",\",\"\"))\n",
    "            name=names[i]\n",
    "            name_source=name.a.string[:-1]\n",
    "            nm.append(name_source)\n",
    "            wikiped=\"https://en.wikipedia.org/wiki/{0}\".format(nm[i])\n",
    "            wiki.append(wikiped)\n",
    "            \n",
    "        \n",
    "        for i in range(3):\n",
    "            print(\"{0}.{1}\".format(i, nm[i]))\n",
    "            \n",
    "        Choice=int(input(\"Enter the serial number shown by the side of the celebrity you want to know about:\"))\n",
    "        if Choice>=0 or Choice<=199:\n",
    "                    print(\"Name : {0} \".format(nm[Choice]))\n",
    "                    print(\"Link to download his image directly : {0} \".format(img[Choice]))\n",
    "                    print(\"Short description on your selected celebrity : {0} \".format(des[Choice]))\n",
    "                    print(\"For more information you can refer to this wiki link : {0} \".format(wiki[Choice]))\n",
    "\n",
    "        \n",
    "        while True:\n",
    "                d=int(input(\"Want to know about somebody  else too : {0}.Yes {1}.No \".format(1,2)))\n",
    "                if d==2:\n",
    "                    print(\"Thank you\")\n",
    "                    break\n",
    "                elif d==1:\n",
    "                    t=int(input(\"Enter the serial number shown by the side of the celebrity you want to know about: \"))\n",
    "                    print(\"Name : {0} \".format(nm[t]))\n",
    "                    print(\"Link to download his image directly : {0} \".format(img[t]))\n",
    "                    print(\"Short description on your selected celebrity : {0} \".format(des[t]))\n",
    "                    print(\"For more information you can refer to this wiki link : {0} \".format(wiki[t]))\n",
    "                    \n",
    "                \n",
    "                \n",
    "            \n",
    "        \n",
    "        j+=1\n",
    "        \n",
    "scrape(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information on evryone into a csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(max_pages):\n",
    "    page = 1\n",
    "    filename = \"celeb.csv\"\n",
    "    f= open(filename, \"w\") \n",
    "    headers = \"Image_Links, Names, Short_Description, For_More_Info\\n\"\n",
    "    f.write(headers)\n",
    "    \n",
    "    while page <= max_pages:\n",
    "        my_url = 'https://www.imdb.com/list/ls068010962/?sort=list_order,asc&mode=detail&page=' + str(page)\n",
    "        uClient = uReq(my_url)\n",
    "        page_html = uClient.read()\n",
    "        uClient.close()\n",
    "        img=[]\n",
    "        nm=[]\n",
    "        des=[]\n",
    "        wiki=[]\n",
    "        \n",
    "        page_soup = soup(page_html, \"html.parser\")\n",
    "        images = page_soup.findAll(\"div\", {\"class\":\"lister-item-image\"})\n",
    "        titles = page_soup.findAll(\"div\", {\"class\":\"lister-item-content\"})\n",
    "        names = page_soup.findAll(\"h3\", {\"class\":\"lister-item-header\"})\n",
    "        descriptions= page_soup.findAll(\"div\",{\"class\":\"lister-item-content\"})\n",
    "        \n",
    "        for i in range(len(titles)):\n",
    "            description= descriptions[i]\n",
    "            x=description.findAll(\"p\")\n",
    "            y=x[1].get_text()[5:75]\n",
    "            z=y.replace(\"\\n\",\"\")\n",
    "            des.append(z.replace(\",\",\"\"))\n",
    "            #print(z)\n",
    "            image=images[i]\n",
    "            image_source=image.a.img[\"src\"]\n",
    "            img.append(image_source.replace(\",\",\"\"))\n",
    "            #print(image_source)\n",
    "            name=names[i]\n",
    "            name_source=name.a.string[:-1]\n",
    "            nm.append(name_source)\n",
    "            #print(name_source)\n",
    "            wikiped=\"https://en.wikipedia.org/wiki/{0}\".format(nm[i])\n",
    "            wiki.append(wikiped)\n",
    "            #print(wikiped)\n",
    "            f.write(image_source.replace(\",\",\"\")+\" , \"+name_source+\" , \"+z.replace(\",\",\"\")+\" , \"+wikiped+\"\\n\")\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        print(\"Required Information from page number {0} has been extracted\".format(page))\n",
    "        \n",
    "        \n",
    "            \n",
    "        page +=1\n",
    "    f.close()\n",
    "            \n",
    "scrape(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
